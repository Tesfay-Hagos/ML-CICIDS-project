{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "header",
            "metadata": {},
            "source": [
                "# CICIDS2017 Data Selection and Exploratory Analysis\n",
                "\n",
                "This notebook performs comprehensive data selection, exploration, and visualization of the CICIDS2017 dataset.\n",
                "\n",
                "## Objectives:\n",
                "1. Load and compare both dataset versions (GeneratedLabelledFlows vs MachineLearningCSV)\n",
                "2. Understand the structure and characteristics of each dataset\n",
                "3. Perform exploratory data analysis with visualizations\n",
                "4. Analyze data quality and class distribution\n",
                "5. Make informed decision on which dataset to use for ML modeling"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "imports",
            "metadata": {},
            "source": [
                "## 1. Setup and Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "install_requirements",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Running on Google Colab - installing packages...\n"
                    ]
                }
            ],
            "source": [
                "# Install required packages if running on Google Colab\n",
                "import sys\n",
                "if 'google.colab' in sys.modules:\n",
                "    print(\"Running on Google Colab - installing packages...\")\n",
                "    !pip install -q pandas numpy matplotlib seaborn plotly\n",
                "else:\n",
                "    print(\"Not running on Colab - assuming packages are installed\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "import_libraries",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úì All libraries imported successfully!\n",
                        "Pandas version: 2.2.2\n",
                        "NumPy version: 2.0.2\n"
                    ]
                }
            ],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from pathlib import Path\n",
                "import warnings\n",
                "import os\n",
                "\n",
                "# Visualization settings\n",
                "plt.style.use('seaborn-v0_8-darkgrid')\n",
                "sns.set_palette(\"husl\")\n",
                "%matplotlib inline\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Display settings\n",
                "pd.set_option('display.max_columns', None)\n",
                "pd.set_option('display.max_rows', 100)\n",
                "pd.set_option('display.width', 1000)\n",
                "\n",
                "print(\"‚úì All libraries imported successfully!\")\n",
                "print(f\"Pandas version: {pd.__version__}\")\n",
                "print(f\"NumPy version: {np.__version__}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "fa936799",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "================================================================================\n",
                        "GOOGLE COLAB SETUP INSTRUCTIONS\n",
                        "================================================================================\n",
                        "\n",
                        "This notebook is configured to work on Google Colab in two ways:\n",
                        "\n",
                        "OPTION 1: Using Google Drive (Recommended)\n",
                        "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
                        "1. Upload the ML-CICIDS-project folder to your Google Drive root\n",
                        "2. When prompted in the next cell, authorize Drive access\n",
                        "3. The notebook will automatically find the project\n",
                        "\n",
                        "OPTION 2: Clone from GitHub\n",
                        "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
                        "1. The notebook will attempt to clone the repo automatically\n",
                        "2. This requires the GitHub repo to be public and accessible\n",
                        "3. Dataset must be uploaded separately to /content/ML-CICIDS-project/\n",
                        "\n",
                        "Note: The next cell will handle the setup automatically.\n",
                        "If you encounter issues, uncomment and run the manual setup below:\n",
                        "\n",
                        "# Manual Setup Option A: Mount Google Drive\n",
                        "# from google.colab import drive\n",
                        "# drive.mount('/content/drive')\n",
                        "\n",
                        "# Manual Setup Option B: Clone from GitHub\n",
                        "# !git clone https://github.com/Tesfay-Hagos/ML-CICIDS-project.git /content/ML-CICIDS-project\n",
                        "\n",
                        "# Manual Setup Option C: Download data_config.py\n",
                        "# !wget -O data_config.py https://raw.githubusercontent.com/Tesfay-Hagos/ML-CICIDS-project/main/data_config.py\n",
                        "\n",
                        "================================================================================\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "# Setup instructions for Google Colab\n",
                "import sys\n",
                "IS_COLAB = 'google.colab' in sys.modules\n",
                "\n",
                "if IS_COLAB:\n",
                "    print(\"=\" * 80)\n",
                "    print(\"GOOGLE COLAB SETUP INSTRUCTIONS\")\n",
                "    print(\"=\" * 80)\n",
                "    print(\"\"\"\n",
                "This notebook is configured to work on Google Colab in two ways:\n",
                "\n",
                "OPTION 1: Using Google Drive (Recommended)\n",
                "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
                "1. Upload the ML-CICIDS-project folder to your Google Drive root\n",
                "2. When prompted in the next cell, authorize Drive access\n",
                "3. The notebook will automatically find the project\n",
                "\n",
                "OPTION 2: Clone from GitHub\n",
                "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
                "1. The notebook will attempt to clone the repo automatically\n",
                "2. This requires the GitHub repo to be public and accessible\n",
                "3. Dataset must be uploaded separately to /content/ML-CICIDS-project/\n",
                "\n",
                "Note: The next cell will handle the setup automatically.\n",
                "If you encounter issues, uncomment and run the manual setup below:\n",
                "\n",
                "# Manual Setup Option A: Mount Google Drive\n",
                "# from google.colab import drive\n",
                "# drive.mount('/content/drive')\n",
                "\n",
                "# Manual Setup Option B: Clone from GitHub\n",
                "# !git clone https://github.com/Tesfay-Hagos/ML-CICIDS-project.git /content/ML-CICIDS-project\n",
                "\n",
                "# Manual Setup Option C: Download data_config.py\n",
                "# !wget -O data_config.py https://raw.githubusercontent.com/Tesfay-Hagos/ML-CICIDS-project/main/data_config.py\n",
                "\"\"\")\n",
                "    print(\"=\" * 80 + \"\\n\")\n",
                "else:\n",
                "    print(\"‚úì Running on local machine (not Colab)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "paths",
            "metadata": {},
            "source": [
                "## 2. Configure Data Paths"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4099d5e0",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "================================================================================\n",
                        "GOOGLE COLAB SETUP INSTRUCTIONS\n",
                        "================================================================================\n",
                        "\n",
                        "This notebook is configured to work on Google Colab in two ways:\n",
                        "\n",
                        "OPTION 1: Using Google Drive (Recommended)\n",
                        "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
                        "1. Upload the ML-CICIDS-project folder to your Google Drive root\n",
                        "2. When prompted in the next cell, authorize Drive access\n",
                        "3. The notebook will automatically find the project\n",
                        "\n",
                        "OPTION 2: Clone from GitHub\n",
                        "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
                        "1. The notebook will attempt to clone the repo automatically\n",
                        "2. This requires the GitHub repo to be public and accessible\n",
                        "3. Dataset must be uploaded separately to /content/ML-CICIDS-project/\n",
                        "\n",
                        "Note: The next cell will handle the setup automatically.\n",
                        "If you encounter issues, uncomment and run the manual setup below:\n",
                        "\n",
                        "\n",
                        "# Manual Setup Option A: Mount Google Drive\n",
                        "# from google.colab import drive\n",
                        "# drive.mount('/content/drive')\n",
                        "\n",
                        "# Manual Setup Option B: Clone from GitHub\n",
                        "# !git clone https://github.com/Tesfay-Hagos/ML-CICIDS-project.git /content/ML-CICIDS-project\n",
                        "\n",
                        "# Manual Setup Option C: Download data_config.py\n",
                        "# !wget -O data_config.py https://raw.githubusercontent.com/Tesfay-Hagos/ML-CICIDS-project/main/data_config.py\n",
                        "\n",
                        "================================================================================\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Colab Setup Helper\n",
                "import sys\n",
                "IS_COLAB = 'google.colab' in sys.modules\n",
                "\n",
                "if IS_COLAB:\n",
                "    print(\"=\" * 80)\n",
                "    print(\"GOOGLE COLAB SETUP - AUTOMATED FALLBACK STRATEGIES\")\n",
                "    print(\"=\" * 80)\n",
                "    print(\"\"\"\n",
                "This cell will automatically try multiple strategies to locate your data:\n",
                "\n",
                "STRATEGY 1: Google Drive\n",
                "   ‚Ä¢ Searches for: /MyDrive/ML-CICIDS-project\n",
                "   ‚Ä¢ Action: Mounts your Google Drive (requires authorization)\n",
                "   ‚Ä¢ Status: Will be attempted first\n",
                "\n",
                "STRATEGY 2: Already Cloned Repository\n",
                "   ‚Ä¢ Searches for: /content/ML-CICIDS-project\n",
                "   ‚Ä¢ Action: Uses repo if already cloned\n",
                "   ‚Ä¢ Status: Checked next\n",
                "\n",
                "STRATEGY 3: Clone from GitHub\n",
                "   ‚Ä¢ Source: https://github.com/Tesfay-Hagos/ML-CICIDS-project\n",
                "   ‚Ä¢ Action: Clones repo to /content/ML-CICIDS-project\n",
                "   ‚Ä¢ Status: Automatic fallback\n",
                "\n",
                "STRATEGY 4: Download data_config.py\n",
                "   ‚Ä¢ Source: GitHub raw URL\n",
                "   ‚Ä¢ Action: Downloads module file\n",
                "   ‚Ä¢ Status: Fallback for strategy 3\n",
                "\n",
                "STRATEGY 5: Create DataConfig Inline\n",
                "   ‚Ä¢ Creates: Self-contained DataConfig class\n",
                "   ‚Ä¢ Action: Works with existing dataset folders\n",
                "   ‚Ä¢ Status: Final fallback (always works!)\n",
                "\n",
                "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
                "\n",
                "IF YOU STILL GET ERRORS:\n",
                "\n",
                "Option A - Use Google Drive (Recommended):\n",
                "  1. Create folder in Google Drive root: ML-CICIDS-project\n",
                "  2. Upload data folders: GeneratedLabelledFlows/ and MachineLearningCSV/\n",
                "  3. Run this cell and authorize Drive access when prompted\n",
                "\n",
                "Option B - Upload to Colab directly:\n",
                "  1. Click folder icon (left sidebar)\n",
                "  2. Upload GeneratedLabelledFlows/ and MachineLearningCSV/\n",
                "  3. Update paths in config if needed\n",
                "\n",
                "Option C - Manual setup:\n",
                "  !mkdir -p /content/ML-CICIDS-project\n",
                "  # Then upload your datasets\n",
                "\n",
                "\"\"\")\n",
                "    print(\"=\" * 80)\n",
                "    print(\"\\nProceeding with automatic setup...\\n\")\n",
                "else:\n",
                "    print(\"‚úì Running on local machine (not Colab)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "setup_paths",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Running on Colab: True\n",
                        "\n",
                        "üìÇ Setting up Colab environment...\n",
                        "\n",
                        "‚ö†Ô∏è  Could not mount Google Drive: mount failed\n",
                        "\n",
                        "üì• Attempting to clone from GitHub...\n",
                        "\n",
                        "‚ö†Ô∏è  Clone succeeded but data_config.py not found\n",
                        "‚ùå ERROR: Could not find project root or data_config.py\n",
                        "\n",
                        "For Colab, you have two options:\n",
                        "1. Mount Google Drive with the project folder\n",
                        "2. Manually upload data_config.py to Colab or update the Google Drive path\n",
                        "\n",
                        "Alternatively, create data_config.py in this Colab cell:\n",
                        "   !wget https://raw.githubusercontent.com/Tesfay-Hagos/ML-CICIDS-project/main/data_config.py\n",
                        "‚ö†Ô∏è  Could not mount Google Drive: mount failed\n",
                        "\n",
                        "üì• Attempting to clone from GitHub...\n",
                        "\n",
                        "‚ö†Ô∏è  Clone succeeded but data_config.py not found\n",
                        "‚ùå ERROR: Could not find project root or data_config.py\n",
                        "\n",
                        "For Colab, you have two options:\n",
                        "1. Mount Google Drive with the project folder\n",
                        "2. Manually upload data_config.py to Colab or update the Google Drive path\n",
                        "\n",
                        "Alternatively, create data_config.py in this Colab cell:\n",
                        "   !wget https://raw.githubusercontent.com/Tesfay-Hagos/ML-CICIDS-project/main/data_config.py\n"
                    ]
                }
            ],
            "source": [
                "# Base path - adjust if needed\n",
                "import sys\n",
                "import os\n",
                "from pathlib import Path\n",
                "\n",
                "# Check if running on Google Colab\n",
                "IS_COLAB = 'google.colab' in sys.modules\n",
                "\n",
                "print(f\"Running on Colab: {IS_COLAB}\")\n",
                "\n",
                "if IS_COLAB:\n",
                "    print(\"\\nüìÇ Setting up Colab environment...\\n\")\n",
                "    \n",
                "    # Strategy 1: Try mounting Google Drive\n",
                "    project_root = None\n",
                "    try:\n",
                "        from google.colab import drive\n",
                "        drive.mount('/content/drive', force_remount=False)\n",
                "        colab_project_path = Path(\"/content/drive/MyDrive/ML-CICIDS-project\")\n",
                "        if colab_project_path.exists() and (colab_project_path / \"data_config.py\").exists():\n",
                "            project_root = colab_project_path\n",
                "            print(f\"‚úì Found project in Google Drive: {project_root}\")\n",
                "        else:\n",
                "            print(\"‚ö†Ô∏è  Project not found in Google Drive\")\n",
                "            print(\"   Looked in: /content/drive/MyDrive/ML-CICIDS-project\")\n",
                "    except Exception as e:\n",
                "        print(f\"‚ö†Ô∏è  Could not mount Google Drive: {str(e)[:100]}\")\n",
                "    \n",
                "    # Strategy 2: Check if data_config.py exists in /content (already cloned)\n",
                "    if project_root is None:\n",
                "        clone_path = Path(\"/content/ML-CICIDS-project\")\n",
                "        if clone_path.exists() and (clone_path / \"data_config.py\").exists():\n",
                "            project_root = clone_path\n",
                "            print(f\"‚úì Found cloned project: {project_root}\")\n",
                "    \n",
                "    # Strategy 3: Try cloning from GitHub again\n",
                "    if project_root is None:\n",
                "        print(\"\\nüì• Attempting to clone from GitHub...\\n\")\n",
                "        try:\n",
                "            os.chdir('/content')\n",
                "            result = os.system('git clone https://github.com/Tesfay-Hagos/ML-CICIDS-project.git 2>/dev/null')\n",
                "            project_root = Path(\"/content/ML-CICIDS-project\")\n",
                "            if (project_root / \"data_config.py\").exists():\n",
                "                print(f\"‚úì Successfully cloned project: {project_root}\")\n",
                "            else:\n",
                "                print(\"‚ö†Ô∏è  Clone succeeded but data_config.py not found in repo\")\n",
                "                project_root = None\n",
                "        except Exception as e:\n",
                "            print(f\"‚ö†Ô∏è  Clone failed: {str(e)[:100]}\")\n",
                "            project_root = None\n",
                "    \n",
                "    # Strategy 4: Create data_config.py inline\n",
                "    if project_root is None:\n",
                "        print(\"\\nüìù Creating data_config.py inline from template...\\n\")\n",
                "        try:\n",
                "            project_root = Path(\"/content/ML-CICIDS-project\")\n",
                "            project_root.mkdir(exist_ok=True)\n",
                "            \n",
                "            # Download or create data_config.py\n",
                "            config_file = project_root / \"data_config.py\"\n",
                "            if not config_file.exists():\n",
                "                os.system(f'wget -q https://raw.githubusercontent.com/Tesfay-Hagos/ML-CICIDS-project/main/data_config.py -O {config_file}')\n",
                "                if config_file.exists() and config_file.stat().st_size > 100:\n",
                "                    print(f\"‚úì Downloaded data_config.py successfully\")\n",
                "                else:\n",
                "                    print(\"‚ö†Ô∏è  Failed to download, creating minimal version...\")\n",
                "                    project_root = None\n",
                "            else:\n",
                "                print(f\"‚úì Found existing data_config.py\")\n",
                "        except Exception as e:\n",
                "            print(f\"‚ö†Ô∏è  Failed to create data_config.py: {str(e)[:100]}\")\n",
                "            project_root = None\n",
                "    \n",
                "    # Strategy 5: Create a minimal data_config module inline\n",
                "    if project_root is None:\n",
                "        print(\"\\nüîß Creating minimal DataConfig class inline...\\n\")\n",
                "        \n",
                "        # Create the DataConfig class directly in memory\n",
                "        from pathlib import Path\n",
                "        import pandas as pd\n",
                "        from typing import Optional, List, Literal\n",
                "        \n",
                "        class DataConfig:\n",
                "            \"\"\"Minimal DataConfig for Colab\"\"\"\n",
                "            \n",
                "            def __init__(self, base_path: Optional[str] = None):\n",
                "                self.base_path = Path(base_path) if base_path else Path(\"/content/ML-CICIDS-project\")\n",
                "                self.flow_base = self.base_path / \"GeneratedLabelledFlows\"\n",
                "                self.ml_base = self.base_path / \"MachineLearningCSV\"\n",
                "                \n",
                "                self.flow_path = self._find_csv_directory(self.flow_base)\n",
                "                self.ml_path = self._find_csv_directory(self.ml_base)\n",
                "                \n",
                "                self.flow_files = self._get_csv_files(self.flow_path) if self.flow_path else []\n",
                "                self.ml_files = self._get_csv_files(self.ml_path) if self.ml_path else []\n",
                "            \n",
                "            def _find_csv_directory(self, parent_dir: Path) -> Optional[Path]:\n",
                "                if not parent_dir.exists():\n",
                "                    return None\n",
                "                csv_files = list(parent_dir.glob(\"*.csv\"))\n",
                "                if csv_files:\n",
                "                    return parent_dir\n",
                "                for subdir in parent_dir.iterdir():\n",
                "                    if subdir.is_dir():\n",
                "                        csv_files = list(subdir.glob(\"*.csv\"))\n",
                "                        if csv_files:\n",
                "                            return subdir\n",
                "                return None\n",
                "            \n",
                "            def _get_csv_files(self, directory: Optional[Path]) -> List[Path]:\n",
                "                if directory is None or not directory.exists():\n",
                "                    return []\n",
                "                return sorted(list(directory.glob(\"*.csv\")))\n",
                "            \n",
                "            def print_summary(self):\n",
                "                print(\"=\" * 80)\n",
                "                print(\"CICIDS2017 DATASET CONFIGURATION (Colab Mode)\")\n",
                "                print(\"=\" * 80)\n",
                "                print(f\"\\nBase directory: {self.base_path}\")\n",
                "                print(f\"Exists: {'‚úì' if self.base_path.exists() else '‚úó'}\")\n",
                "                \n",
                "                print(\"\\n\" + \"-\" * 80)\n",
                "                print(\"GeneratedLabelledFlows Dataset\")\n",
                "                print(\"-\" * 80)\n",
                "                if self.flow_path:\n",
                "                    print(f\"‚úì Path: {self.flow_path}\")\n",
                "                    print(f\"  Files: {len(self.flow_files)}\")\n",
                "                else:\n",
                "                    print(f\"‚úó Not found\")\n",
                "                \n",
                "                print(\"\\n\" + \"-\" * 80)\n",
                "                print(\"MachineLearningCSV Dataset\")\n",
                "                print(\"-\" * 80)\n",
                "                if self.ml_path:\n",
                "                    print(f\"‚úì Path: {self.ml_path}\")\n",
                "                    print(f\"  Files: {len(self.ml_files)}\")\n",
                "                else:\n",
                "                    print(f\"‚úó Not found\")\n",
                "                print(\"\\n\" + \"=\" * 80)\n",
                "            \n",
                "            def get_file_path(self, filename: str, dataset: Literal['flow', 'ml'] = 'ml') -> Optional[Path]:\n",
                "                base_path = self.ml_path if dataset == 'ml' else self.flow_path\n",
                "                file_list = self.ml_files if dataset == 'ml' else self.flow_files\n",
                "                \n",
                "                if base_path is None:\n",
                "                    return None\n",
                "                \n",
                "                file_path = base_path / filename\n",
                "                if file_path.exists():\n",
                "                    return file_path\n",
                "                \n",
                "                for f in file_list:\n",
                "                    if f.name.lower() == filename.lower():\n",
                "                        return f\n",
                "                return None\n",
                "            \n",
                "            def load_file(self, filename: str, dataset: Literal['flow', 'ml'] = 'ml', \n",
                "                         nrows: Optional[int] = None, **kwargs) -> pd.DataFrame:\n",
                "                file_path = self.get_file_path(filename, dataset)\n",
                "                if file_path is None:\n",
                "                    raise FileNotFoundError(f\"File '{filename}' not found in {dataset} dataset\")\n",
                "                \n",
                "                print(f\"Loading: {file_path.name}\")\n",
                "                if nrows:\n",
                "                    print(f\"  Reading first {nrows:,} rows...\")\n",
                "                df = pd.read_csv(file_path, nrows=nrows, **kwargs)\n",
                "                print(f\"  ‚úì Loaded {len(df):,} rows √ó {len(df.columns)} columns\")\n",
                "                return df\n",
                "            \n",
                "            def load_all_files(self, dataset: Literal['flow', 'ml'] = 'ml', \n",
                "                              nrows: Optional[int] = None, **kwargs) -> pd.DataFrame:\n",
                "                file_list = self.ml_files if dataset == 'ml' else self.flow_files\n",
                "                if not file_list:\n",
                "                    raise ValueError(f\"No files found in {dataset} dataset\")\n",
                "                \n",
                "                print(f\"Loading {len(file_list)} files from {dataset} dataset...\")\n",
                "                dfs = []\n",
                "                for file_path in file_list:\n",
                "                    print(f\"  Processing: {file_path.name}\")\n",
                "                    df = pd.read_csv(file_path, nrows=nrows, **kwargs)\n",
                "                    print(f\"    ‚úì {len(df):,} rows\")\n",
                "                    dfs.append(df)\n",
                "                \n",
                "                combined = pd.concat(dfs, ignore_index=True)\n",
                "                print(f\"\\n‚úì Combined: {len(combined):,} rows √ó {len(combined.columns)} columns\")\n",
                "                return combined\n",
                "        \n",
                "        config = DataConfig()\n",
                "        print(f\"‚úì Created inline DataConfig class\")\n",
                "else:\n",
                "    # Local environment (not Colab)\n",
                "    current_dir = Path(os.getcwd())\n",
                "    project_root = None\n",
                "    \n",
                "    # Search current and parent directories for data_config.py\n",
                "    for parent in [current_dir] + list(current_dir.parents):\n",
                "        if (parent / \"data_config.py\").exists():\n",
                "            project_root = parent\n",
                "            break\n",
                "    \n",
                "    # Fallback to known path if automatic discovery fails\n",
                "    if project_root is None:\n",
                "        known_path = Path(\"/home/tesfayh/Artificial_inteligence/ML/CICDS/ML-CICIDS-project/\")\n",
                "        if known_path.exists() and (known_path / \"data_config.py\").exists():\n",
                "            project_root = known_path\n",
                "\n",
                "# For local environment, import the module version\n",
                "if not IS_COLAB and project_root:\n",
                "    if str(project_root) not in sys.path:\n",
                "        sys.path.insert(0, str(project_root))\n",
                "    print(f\"‚úì Added to path: {project_root}\")\n",
                "    \n",
                "    from data_config import DataConfig\n",
                "    config = DataConfig(base_path=str(project_root))\n",
                "elif IS_COLAB and 'config' not in locals():\n",
                "    print(\"\\n‚ùå ERROR: Could not initialize DataConfig\")\n",
                "    print(\"\\nPlease try one of these options:\")\n",
                "    print(\"1. Ensure your Google Drive has the project folder at /MyDrive/ML-CICIDS-project\")\n",
                "    print(\"2. Upload dataset folders to Colab manually\")\n",
                "    print(\"3. Contact the maintainer for troubleshooting\")\n",
                "    config = None\n",
                "\n",
                "# Only proceed if config was successfully created\n",
                "if config is not None:\n",
                "    config.print_summary()\n",
                "    \n",
                "    # Get file lists from config\n",
                "    flow_files = config.flow_files\n",
                "    ml_files = config.ml_files\n",
                "    \n",
                "    # Display file names\n",
                "    if flow_files:\n",
                "        print(\"\\nüìÑ Available files (GeneratedLabelledFlows):\")\n",
                "        for i, f in enumerate(flow_files, 1):\n",
                "            print(f\"   {i}. {f.name}\")\n",
                "    \n",
                "    if ml_files:\n",
                "        print(\"\\nüìÑ Available files (MachineLearningCSV):\")\n",
                "        for i, f in enumerate(ml_files, 1):\n",
                "            print(f\"   {i}. {f.name}\")\n",
                "    \n",
                "    if not flow_files and not ml_files:\n",
                "        print(\"\\n‚ö†Ô∏è  WARNING: No CSV files found in dataset directories\")\n",
                "        print(\"   Please ensure the dataset folders contain CSV files\")\n",
                "else:\n",
                "    flow_files = []\n",
                "    ml_files = []\n",
                "    print(\"\\n‚ö†Ô∏è  Configuration failed - cannot proceed with data loading\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "dataset_comparison",
            "metadata": {},
            "source": [
                "## 3. Dataset Comparison: Structure and Columns"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "compare_structure",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading sample files for comparison...\n",
                        "\n"
                    ]
                },
                {
                    "ename": "NameError",
                    "evalue": "name 'config' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
                        "\u001b[0;32m/tmp/ipython-input-3833254190.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading sample files for comparison...\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Use config to load files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mflow_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'flow'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mml_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ml'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
                    ]
                }
            ],
            "source": [
                "# Load first file from each dataset to compare structure\n",
                "sample_file = \"Monday-WorkingHours.pcap_ISCX.csv\"\n",
                "\n",
                "print(\"Loading sample files for comparison...\\n\")\n",
                "# Use config to load files\n",
                "flow_sample = config.load_file(sample_file, dataset='flow', nrows=1000)\n",
                "ml_sample = config.load_file(sample_file, dataset='ml', nrows=1000)\n",
                "\n",
                "print(\"=\" * 80)\n",
                "print(\"DATASET STRUCTURE COMPARISON\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "print(f\"\\nüìä GeneratedLabelledFlows:\")\n",
                "print(f\"   - Shape: {flow_sample.shape}\")\n",
                "print(f\"   - Columns: {len(flow_sample.columns)}\")\n",
                "print(f\"   - Memory: {flow_sample.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
                "\n",
                "print(f\"\\nüìä MachineLearningCSV:\")\n",
                "print(f\"   - Shape: {ml_sample.shape}\")\n",
                "print(f\"   - Columns: {len(ml_sample.columns)}\")\n",
                "print(f\"   - Memory: {ml_sample.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
                "\n",
                "# Column differences\n",
                "flow_cols = set(flow_sample.columns)\n",
                "ml_cols = set(ml_sample.columns)\n",
                "\n",
                "common_cols = flow_cols & ml_cols\n",
                "flow_only = flow_cols - ml_cols\n",
                "ml_only = ml_cols - flow_cols\n",
                "\n",
                "print(f\"\\nüîç Column Analysis:\")\n",
                "print(f\"   - Common columns: {len(common_cols)}\")\n",
                "print(f\"   - Only in GeneratedLabelledFlows: {len(flow_only)}\")\n",
                "print(f\"   - Only in MachineLearningCSV: {len(ml_only)}\")\n",
                "\n",
                "if flow_only:\n",
                "    print(f\"\\n   Columns ONLY in GeneratedLabelledFlows:\")\n",
                "    for col in sorted(flow_only):\n",
                "        print(f\"      ‚Ä¢ {col}\")\n",
                "\n",
                "if ml_only:\n",
                "    print(f\"\\n   Columns ONLY in MachineLearningCSV:\")\n",
                "    for col in sorted(ml_only):\n",
                "        print(f\"      ‚Ä¢ {col}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "visualize_columns",
            "metadata": {},
            "source": [
                "### Visualize Column Differences"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "plot_column_comparison",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create visualization of column distribution\n",
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
                "\n",
                "# Pie chart showing column composition\n",
                "column_data = [len(common_cols), len(flow_only), len(ml_only)]\n",
                "labels = ['Common Columns', 'Flow Only', 'ML Only']\n",
                "colors = ['#2ecc71', '#3498db', '#e74c3c']\n",
                "explode = (0.05, 0.05, 0.05)\n",
                "\n",
                "ax1.pie(column_data, labels=labels, colors=colors, autopct='%1.1f%%', \n",
                "        startangle=90, explode=explode, shadow=True)\n",
                "ax1.set_title('Column Distribution Across Datasets', fontsize=14, fontweight='bold')\n",
                "\n",
                "# Bar chart comparing total columns\n",
                "datasets = ['GeneratedLabelledFlows', 'MachineLearningCSV']\n",
                "column_counts = [len(flow_sample.columns), len(ml_sample.columns)]\n",
                "bars = ax2.bar(datasets, column_counts, color=['#3498db', '#e74c3c'], alpha=0.7, edgecolor='black')\n",
                "\n",
                "# Add value labels on bars\n",
                "for bar in bars:\n",
                "    height = bar.get_height()\n",
                "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
                "            f'{int(height)}',\n",
                "            ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
                "\n",
                "ax2.set_ylabel('Number of Columns', fontsize=12)\n",
                "ax2.set_title('Total Columns Per Dataset', fontsize=14, fontweight='bold')\n",
                "ax2.grid(axis='y', alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "data_overview",
            "metadata": {},
            "source": [
                "## 4. Data Overview and Statistics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "load_full_monday",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load full Monday dataset for detailed analysis\n",
                "print(\"Loading Monday dataset (full) for detailed analysis...\\n\")\n",
                "\n",
                "# Using MachineLearningCSV as it's preprocessed for ML\n",
                "monday_data = config.load_file(sample_file, dataset='ml')\n",
                "\n",
                "print(\"‚úì Data loaded successfully!\\n\")\n",
                "print(\"=\" * 80)\n",
                "print(\"DATASET OVERVIEW\")\n",
                "print(\"=\" * 80)\n",
                "print(f\"Shape: {monday_data.shape[0]:,} rows √ó {monday_data.shape[1]} columns\")\n",
                "print(f\"Memory usage: {monday_data.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
                "print(f\"\\nFirst few rows:\")\n",
                "display(monday_data.head())\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"DATA TYPES\")\n",
                "print(\"=\"*80)\n",
                "print(monday_data.dtypes.value_counts())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "basic_stats",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Statistical summary\n",
                "print(\"üìä Statistical Summary of Numerical Features:\\n\")\n",
                "display(monday_data.describe())"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "missing_data",
            "metadata": {},
            "source": [
                "## 5. Data Quality Assessment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "check_missing",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check for missing values\n",
                "missing_data = monday_data.isnull().sum()\n",
                "missing_percent = (missing_data / len(monday_data)) * 100\n",
                "missing_df = pd.DataFrame({\n",
                "    'Missing Count': missing_data,\n",
                "    'Percentage': missing_percent\n",
                "})\n",
                "missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
                "\n",
                "print(\"=\" * 80)\n",
                "print(\"MISSING DATA ANALYSIS\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "if len(missing_df) > 0:\n",
                "    print(f\"\\n‚ö†Ô∏è  Found {len(missing_df)} columns with missing values:\\n\")\n",
                "    display(missing_df.head(20))\n",
                "else:\n",
                "    print(\"\\n‚úì No missing values found!\")\n",
                "\n",
                "# Check for infinite values in numerical columns\n",
                "numerical_cols = monday_data.select_dtypes(include=[np.number]).columns\n",
                "inf_counts = {}\n",
                "\n",
                "for col in numerical_cols:\n",
                "    inf_count = np.isinf(monday_data[col]).sum()\n",
                "    if inf_count > 0:\n",
                "        inf_counts[col] = inf_count\n",
                "\n",
                "if inf_counts:\n",
                "    print(f\"\\n‚ö†Ô∏è  Found infinite values in {len(inf_counts)} columns:\")\n",
                "    for col, count in sorted(inf_counts.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
                "        print(f\"   ‚Ä¢ {col}: {count:,} infinite values\")\n",
                "else:\n",
                "    print(\"\\n‚úì No infinite values found!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "visualize_missing",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize missing data\n",
                "if len(missing_df) > 0:\n",
                "    fig, ax = plt.subplots(figsize=(12, 6))\n",
                "    \n",
                "    top_missing = missing_df.head(15)\n",
                "    bars = ax.barh(range(len(top_missing)), top_missing['Percentage'], color='coral', edgecolor='black')\n",
                "    ax.set_yticks(range(len(top_missing)))\n",
                "    ax.set_yticklabels(top_missing.index)\n",
                "    ax.set_xlabel('Missing Percentage (%)', fontsize=12)\n",
                "    ax.set_title('Top 15 Columns with Missing Data', fontsize=14, fontweight='bold')\n",
                "    ax.grid(axis='x', alpha=0.3)\n",
                "    \n",
                "    # Add percentage labels\n",
                "    for i, bar in enumerate(bars):\n",
                "        width = bar.get_width()\n",
                "        ax.text(width, bar.get_y() + bar.get_height()/2.,\n",
                "               f'{width:.2f}%',\n",
                "               ha='left', va='center', fontsize=9)\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "else:\n",
                "    print(\"‚úì No missing data to visualize!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "label_analysis",
            "metadata": {},
            "source": [
                "## 6. Label Distribution Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "analyze_labels",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze label distribution\n",
                "label_col = ' Label' if ' Label' in monday_data.columns else 'Label'\n",
                "\n",
                "label_counts = monday_data[label_col].value_counts()\n",
                "label_percentages = monday_data[label_col].value_counts(normalize=True) * 100\n",
                "\n",
                "label_summary = pd.DataFrame({\n",
                "    'Count': label_counts,\n",
                "    'Percentage': label_percentages\n",
                "})\n",
                "\n",
                "print(\"=\" * 80)\n",
                "print(\"LABEL DISTRIBUTION (Monday Dataset)\")\n",
                "print(\"=\" * 80)\n",
                "print(f\"\\nTotal unique labels: {len(label_counts)}\\n\")\n",
                "display(label_summary)\n",
                "\n",
                "# Check for class imbalance\n",
                "if len(label_counts) > 1:\n",
                "    imbalance_ratio = label_counts.max() / label_counts.min()\n",
                "    print(f\"\\n‚öñÔ∏è  Class Imbalance Ratio: {imbalance_ratio:.2f}:1\")\n",
                "    if imbalance_ratio > 10:\n",
                "        print(\"   ‚ö†Ô∏è  Significant class imbalance detected! Consider using:\")\n",
                "        print(\"      ‚Ä¢ SMOTE (Synthetic Minority Over-sampling)\")\n",
                "        print(\"      ‚Ä¢ Class weights in model training\")\n",
                "        print(\"      ‚Ä¢ Stratified sampling\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "plot_labels",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize label distribution\n",
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
                "\n",
                "# Bar plot\n",
                "colors_list = plt.cm.Set3(range(len(label_counts)))\n",
                "bars = ax1.bar(range(len(label_counts)), label_counts.values, color=colors_list, edgecolor='black', alpha=0.8)\n",
                "ax1.set_xticks(range(len(label_counts)))\n",
                "ax1.set_xticklabels(label_counts.index, rotation=45, ha='right')\n",
                "ax1.set_ylabel('Count', fontsize=12)\n",
                "ax1.set_title('Label Distribution (Count)', fontsize=14, fontweight='bold')\n",
                "ax1.grid(axis='y', alpha=0.3)\n",
                "\n",
                "# Add count labels on bars\n",
                "for bar in bars:\n",
                "    height = bar.get_height()\n",
                "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
                "            f'{int(height):,}',\n",
                "            ha='center', va='bottom', fontsize=9, rotation=0)\n",
                "\n",
                "# Pie chart\n",
                "if len(label_counts) <= 10:  # Only show pie chart if not too many labels\n",
                "    wedges, texts, autotexts = ax2.pie(label_counts.values, labels=label_counts.index, \n",
                "                                         autopct='%1.1f%%', startangle=90, \n",
                "                                         colors=colors_list, \n",
                "                                         explode=[0.05] * len(label_counts))\n",
                "    ax2.set_title('Label Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
                "    \n",
                "    # Make percentage text more readable\n",
                "    for autotext in autotexts:\n",
                "        autotext.set_color('white')\n",
                "        autotext.set_fontweight('bold')\n",
                "else:\n",
                "    # If too many labels, show log scale bar plot\n",
                "    label_counts.plot(kind='bar', ax=ax2, color=colors_list, edgecolor='black', alpha=0.8, logy=True)\n",
                "    ax2.set_title('Label Distribution (Log Scale)', fontsize=14, fontweight='bold')\n",
                "    ax2.set_ylabel('Count (log scale)', fontsize=12)\n",
                "    ax2.set_xlabel('Label', fontsize=12)\n",
                "    ax2.tick_params(axis='x', rotation=45)\n",
                "    ax2.grid(axis='y', alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "all_files_labels",
            "metadata": {},
            "source": [
                "## 7. Complete Dataset Label Distribution"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "analyze_all_files",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze labels across all files\n",
                "print(\"Loading all files to analyze complete label distribution...\\n\")\n",
                "print(\"This may take a moment...\\n\")\n",
                "\n",
                "all_labels = []\n",
                "file_info = []\n",
                "\n",
                "for csv_file in ml_files:\n",
                "    print(f\"Processing: {csv_file.name}\")\n",
                "    try:\n",
                "        df = pd.read_csv(csv_file)\n",
                "        labels = df[label_col].value_counts()\n",
                "        \n",
                "        file_info.append({\n",
                "            'File': csv_file.name,\n",
                "            'Total Rows': len(df),\n",
                "            'Unique Labels': len(labels),\n",
                "            'Labels': ', '.join(labels.index.tolist())\n",
                "        })\n",
                "        \n",
                "        all_labels.extend(df[label_col].tolist())\n",
                "    except Exception as e:\n",
                "        print(f\"   ‚ö†Ô∏è  Error: {e}\")\n",
                "\n",
                "# Create summary DataFrame\n",
                "file_summary = pd.DataFrame(file_info)\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"FILE-LEVEL SUMMARY\")\n",
                "print(\"=\"*80)\n",
                "display(file_summary)\n",
                "\n",
                "# Overall label distribution\n",
                "overall_labels = pd.Series(all_labels).value_counts()\n",
                "overall_percentages = pd.Series(all_labels).value_counts(normalize=True) * 100\n",
                "\n",
                "overall_summary = pd.DataFrame({\n",
                "    'Count': overall_labels,\n",
                "    'Percentage': overall_percentages\n",
                "})\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"OVERALL LABEL DISTRIBUTION (All Files Combined)\")\n",
                "print(\"=\"*80)\n",
                "print(f\"Total samples: {len(all_labels):,}\")\n",
                "print(f\"Unique labels: {len(overall_labels)}\\n\")\n",
                "display(overall_summary)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "plot_overall_labels",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize overall label distribution\n",
                "fig, ax = plt.subplots(figsize=(14, 7))\n",
                "\n",
                "colors_overall = plt.cm.tab20(range(len(overall_labels)))\n",
                "bars = ax.bar(range(len(overall_labels)), overall_labels.values, \n",
                "              color=colors_overall, edgecolor='black', alpha=0.8)\n",
                "\n",
                "ax.set_xticks(range(len(overall_labels)))\n",
                "ax.set_xticklabels(overall_labels.index, rotation=45, ha='right', fontsize=10)\n",
                "ax.set_ylabel('Count', fontsize=12, fontweight='bold')\n",
                "ax.set_xlabel('Attack Type', fontsize=12, fontweight='bold')\n",
                "ax.set_title('Complete CICIDS2017 Dataset - Label Distribution (All Files)', \n",
                "             fontsize=16, fontweight='bold', pad=20)\n",
                "ax.grid(axis='y', alpha=0.3)\n",
                "ax.set_yscale('log')  # Log scale for better visibility\n",
                "\n",
                "# Add count labels\n",
                "for i, bar in enumerate(bars):\n",
                "    height = bar.get_height()\n",
                "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
                "           f'{int(overall_labels.values[i]):,}',\n",
                "           ha='center', va='bottom', fontsize=8, rotation=45)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "feature_analysis",
            "metadata": {},
            "source": [
                "## 8. Feature Distribution Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "analyze_features",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Select numerical features for analysis (excluding label)\n",
                "feature_cols = [col for col in monday_data.select_dtypes(include=[np.number]).columns \n",
                "                if col != label_col]\n",
                "\n",
                "print(f\"Analyzing {len(feature_cols)} numerical features...\\n\")\n",
                "\n",
                "# Sample of key features to visualize\n",
                "key_features = [\n",
                "    'Flow Duration',\n",
                "    ' Total Fwd Packets',\n",
                "    ' Total Backward Packets',\n",
                "    'Total Length of Fwd Packets',\n",
                "    ' Total Length of Bwd Packets',\n",
                "    ' Flow Bytes/s',\n",
                "    ' Flow Packets/s'\n",
                "]\n",
                "\n",
                "# Filter to features that exist\n",
                "available_features = [f for f in key_features if f in monday_data.columns]\n",
                "\n",
                "if len(available_features) >= 4:\n",
                "    # Plot distributions\n",
                "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
                "    axes = axes.ravel()\n",
                "    \n",
                "    for i, feature in enumerate(available_features[:4]):\n",
                "        # Remove infinite values for visualization\n",
                "        data = monday_data[feature].replace([np.inf, -np.inf], np.nan).dropna()\n",
                "        \n",
                "        axes[i].hist(data, bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
                "        axes[i].set_xlabel(feature, fontsize=11)\n",
                "        axes[i].set_ylabel('Frequency', fontsize=11)\n",
                "        axes[i].set_title(f'Distribution: {feature}', fontsize=12, fontweight='bold')\n",
                "        axes[i].grid(alpha=0.3)\n",
                "        \n",
                "        # Add statistics\n",
                "        mean_val = data.mean()\n",
                "        median_val = data.median()\n",
                "        axes[i].axvline(mean_val, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_val:.2e}')\n",
                "        axes[i].axvline(median_val, color='green', linestyle='--', linewidth=2, label=f'Median: {median_val:.2e}')\n",
                "        axes[i].legend()\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "else:\n",
                "    print(\"Not enough features available for visualization.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "correlation",
            "metadata": {},
            "source": [
                "## 9. Feature Correlation Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "correlation_matrix",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate correlation matrix for a subset of features\n",
                "print(\"Computing correlation matrix...\\n\")\n",
                "\n",
                "# Select subset of features (to avoid overwhelming visualization)\n",
                "sample_features = monday_data[available_features[:10]].replace([np.inf, -np.inf], np.nan)\n",
                "correlation_matrix = sample_features.corr()\n",
                "\n",
                "# Plot heatmap\n",
                "fig, ax = plt.subplots(figsize=(12, 10))\n",
                "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
                "            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8},\n",
                "            ax=ax)\n",
                "ax.set_title('Feature Correlation Matrix (Sample Features)', fontsize=14, fontweight='bold', pad=20)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Find highly correlated features\n",
                "high_corr = []\n",
                "for i in range(len(correlation_matrix.columns)):\n",
                "    for j in range(i+1, len(correlation_matrix.columns)):\n",
                "        if abs(correlation_matrix.iloc[i, j]) > 0.8:\n",
                "            high_corr.append({\n",
                "                'Feature 1': correlation_matrix.columns[i],\n",
                "                'Feature 2': correlation_matrix.columns[j],\n",
                "                'Correlation': correlation_matrix.iloc[i, j]\n",
                "            })\n",
                "\n",
                "if high_corr:\n",
                "    print(\"\\n‚ö†Ô∏è  Highly correlated features (|r| > 0.8):\")\n",
                "    high_corr_df = pd.DataFrame(high_corr).sort_values('Correlation', key=abs, ascending=False)\n",
                "    display(high_corr_df)\n",
                "    print(\"\\nüí° Consider removing one feature from each pair to reduce multicollinearity.\")\n",
                "else:\n",
                "    print(\"\\n‚úì No highly correlated features found (|r| > 0.8)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "recommendations",
            "metadata": {},
            "source": [
                "## 10. Dataset Selection Recommendation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "final_recommendation",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 80)\n",
                "print(\"DATASET SELECTION RECOMMENDATION\")\n",
                "print(\"=\" * 80)\n",
                "print(\"\"\"\n",
                "Based on the analysis, here are the recommendations:\n",
                "\n",
                "üìå FOR MACHINE LEARNING MODEL TRAINING:\n",
                "   ‚úì USE: MachineLearningCSV/MachineLearningCVE\n",
                "   \n",
                "   Reasons:\n",
                "   ‚Ä¢ Preprocessed and optimized for ML algorithms\n",
                "   ‚Ä¢ Removes identifying information (IPs, ports, timestamps)\n",
                "   ‚Ä¢ Focuses on statistical flow features\n",
                "   ‚Ä¢ Privacy-preserving (no personal/network identifiers)\n",
                "   ‚Ä¢ Smaller memory footprint\n",
                "   ‚Ä¢ Industry standard for intrusion detection research\n",
                "\n",
                "üìå FOR NETWORK FORENSICS & DETAILED ANALYSIS:\n",
                "   ‚úì USE: GeneratedLabelledFlows\n",
                "   \n",
                "   Reasons:\n",
                "   ‚Ä¢ Contains complete flow information\n",
                "   ‚Ä¢ Includes Flow ID, Source/Dest IPs, Ports, Timestamps\n",
                "   ‚Ä¢ Useful for tracking specific flows\n",
                "   ‚Ä¢ Better for investigating attack patterns\n",
                "   ‚Ä¢ Correlate with original PCAP files\n",
                "\n",
                "üìä DATASET STATISTICS:\n",
                "   ‚Ä¢ Total Samples: {:,}\n",
                "   ‚Ä¢ Unique Attack Types: {}\n",
                "   ‚Ä¢ Features (ML version): {}\n",
                "   ‚Ä¢ Class Imbalance: Present (consider SMOTE or class weights)\n",
                "\n",
                "‚ö†Ô∏è  KEY CONSIDERATIONS:\n",
                "   ‚Ä¢ Significant class imbalance exists - use appropriate techniques\n",
                "   ‚Ä¢ Some features contain infinite values - handle during preprocessing\n",
                "   ‚Ä¢ High correlation between some features - consider dimensionality reduction\n",
                "   ‚Ä¢ Stratified sampling recommended for train/test split\n",
                "\n",
                "üéØ NEXT STEPS:\n",
                "   1. Data Preprocessing (handle infinities, normalize features)\n",
                "   2. Feature Selection/Engineering\n",
                "   3. Handle Class Imbalance (SMOTE, class weights)\n",
                "   4. Train/Test Split (stratified)\n",
                "   5. Model Selection and Training\n",
                "   6. Evaluation with appropriate metrics (F1, Precision, Recall)\n",
                "\"\"\".format(\n",
                "    len(all_labels),\n",
                "    len(overall_labels),\n",
                "    len(ml_sample.columns)\n",
                "))\n",
                "\n",
                "print(\"=\" * 80)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "export_summary",
            "metadata": {},
            "source": [
                "## 11. Export Analysis Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "export_results",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create summary report\n",
                "summary_report = {\n",
                "    'analysis_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
                "    'total_samples': len(all_labels),\n",
                "    'unique_labels': len(overall_labels),\n",
                "    'label_distribution': overall_summary.to_dict(),\n",
                "    'files_analyzed': len(ml_files),\n",
                "    'file_summary': file_summary.to_dict(),\n",
                "    'recommended_dataset': 'MachineLearningCSV/MachineLearningCVE',\n",
                "    'features_count': len(ml_sample.columns)\n",
                "}\n",
                "\n",
                "# Save to JSON\n",
                "import json\n",
                "output_path = config.base_path / 'data_analysis_summary.json'\n",
                "\n",
                "with open(output_path, 'w') as f:\n",
                "    json.dump(summary_report, f, indent=2, default=str)\n",
                "\n",
                "print(f\"‚úì Analysis summary saved to: {output_path}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
