{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "header",
            "metadata": {},
            "source": [
                "# CICIDS2017 Data Selection and Exploratory Analysis\n",
                "\n",
                "This notebook performs comprehensive data selection, exploration, and visualization of the CICIDS2017 dataset.\n",
                "\n",
                "## Objectives:\n",
                "1. Load and compare both dataset versions (GeneratedLabelledFlows vs MachineLearningCSV)\n",
                "2. Understand the structure and characteristics of each dataset\n",
                "3. Perform exploratory data analysis with visualizations\n",
                "4. Analyze data quality and class distribution\n",
                "5. Make informed decision on which dataset to use for ML modeling"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "imports",
            "metadata": {},
            "source": [
                "## 1. Setup and Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "install_requirements",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Running on Google Colab - installing packages...\n"
                    ]
                }
            ],
            "source": [
                "# Install required packages if running on Google Colab\n",
                "import sys\n",
                "if 'google.colab' in sys.modules:\n",
                "    print(\"Running on Google Colab - installing packages...\")\n",
                "    !pip install -q pandas numpy matplotlib seaborn plotly\n",
                "else:\n",
                "    print(\"Not running on Colab - assuming packages are installed\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "import_libraries",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úì All libraries imported successfully!\n",
                        "Pandas version: 2.2.2\n",
                        "NumPy version: 2.0.2\n"
                    ]
                }
            ],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from pathlib import Path\n",
                "import warnings\n",
                "import os\n",
                "\n",
                "# Visualization settings\n",
                "plt.style.use('seaborn-v0_8-darkgrid')\n",
                "sns.set_palette(\"husl\")\n",
                "%matplotlib inline\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Display settings\n",
                "pd.set_option('display.max_columns', None)\n",
                "pd.set_option('display.max_rows', 100)\n",
                "pd.set_option('display.width', 1000)\n",
                "\n",
                "print(\"‚úì All libraries imported successfully!\")\n",
                "print(f\"Pandas version: {pd.__version__}\")\n",
                "print(f\"NumPy version: {np.__version__}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "fa936799",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "================================================================================\n",
                        "GOOGLE COLAB SETUP INSTRUCTIONS\n",
                        "================================================================================\n",
                        "\n",
                        "This notebook is configured to work on Google Colab in two ways:\n",
                        "\n",
                        "OPTION 1: Using Google Drive (Recommended)\n",
                        "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
                        "1. Upload the ML-CICIDS-project folder to your Google Drive root\n",
                        "2. When prompted in the next cell, authorize Drive access\n",
                        "3. The notebook will automatically find the project\n",
                        "\n",
                        "OPTION 2: Clone from GitHub\n",
                        "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
                        "1. The notebook will attempt to clone the repo automatically\n",
                        "2. This requires the GitHub repo to be public and accessible\n",
                        "3. Dataset must be uploaded separately to /content/ML-CICIDS-project/\n",
                        "\n",
                        "Note: The next cell will handle the setup automatically.\n",
                        "If you encounter issues, uncomment and run the manual setup below:\n",
                        "\n",
                        "# Manual Setup Option A: Mount Google Drive\n",
                        "# from google.colab import drive\n",
                        "# drive.mount('/content/drive')\n",
                        "\n",
                        "# Manual Setup Option B: Clone from GitHub\n",
                        "# !git clone https://github.com/Tesfay-Hagos/ML-CICIDS-project.git /content/ML-CICIDS-project\n",
                        "\n",
                        "# Manual Setup Option C: Download data_config.py\n",
                        "# !wget -O data_config.py https://raw.githubusercontent.com/Tesfay-Hagos/ML-CICIDS-project/main/data_config.py\n",
                        "\n",
                        "================================================================================\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "# Setup instructions for Google Colab\n",
                "import sys\n",
                "IS_COLAB = 'google.colab' in sys.modules\n",
                "\n",
                "if IS_COLAB:\n",
                "    print(\"=\" * 80)\n",
                "    print(\"GOOGLE COLAB SETUP INSTRUCTIONS\")\n",
                "    print(\"=\" * 80)\n",
                "    print(\"\"\"\n",
                "This notebook is configured to work on Google Colab in two ways:\n",
                "\n",
                "OPTION 1: Using Google Drive (Recommended)\n",
                "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
                "1. Upload the ML-CICIDS-project folder to your Google Drive root\n",
                "2. When prompted in the next cell, authorize Drive access\n",
                "3. The notebook will automatically find the project\n",
                "\n",
                "OPTION 2: Clone from GitHub\n",
                "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
                "1. The notebook will attempt to clone the repo automatically\n",
                "2. This requires the GitHub repo to be public and accessible\n",
                "3. Dataset must be uploaded separately to /content/ML-CICIDS-project/\n",
                "\n",
                "Note: The next cell will handle the setup automatically.\n",
                "If you encounter issues, uncomment and run the manual setup below:\n",
                "\n",
                "# Manual Setup Option A: Mount Google Drive\n",
                "# from google.colab import drive\n",
                "# drive.mount('/content/drive')\n",
                "\n",
                "# Manual Setup Option B: Clone from GitHub\n",
                "# !git clone https://github.com/Tesfay-Hagos/ML-CICIDS-project.git /content/ML-CICIDS-project\n",
                "\n",
                "# Manual Setup Option C: Download data_config.py\n",
                "# !wget -O data_config.py https://raw.githubusercontent.com/Tesfay-Hagos/ML-CICIDS-project/main/data_config.py\n",
                "\"\"\")\n",
                "    print(\"=\" * 80 + \"\\n\")\n",
                "else:\n",
                "    print(\"‚úì Running on local machine (not Colab)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "paths",
            "metadata": {},
            "source": [
                "## 2. Configure Data Paths"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "4099d5e0",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "================================================================================\n",
                        "GOOGLE COLAB SETUP INSTRUCTIONS\n",
                        "================================================================================\n",
                        "\n",
                        "This notebook is configured to work on Google Colab in two ways:\n",
                        "\n",
                        "OPTION 1: Using Google Drive (Recommended)\n",
                        "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
                        "1. Upload the ML-CICIDS-project folder to your Google Drive root\n",
                        "2. When prompted in the next cell, authorize Drive access\n",
                        "3. The notebook will automatically find the project\n",
                        "\n",
                        "OPTION 2: Clone from GitHub\n",
                        "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
                        "1. The notebook will attempt to clone the repo automatically\n",
                        "2. This requires the GitHub repo to be public and accessible\n",
                        "3. Dataset must be uploaded separately to /content/ML-CICIDS-project/\n",
                        "\n",
                        "Note: The next cell will handle the setup automatically.\n",
                        "If you encounter issues, uncomment and run the manual setup below:\n",
                        "\n",
                        "\n",
                        "# Manual Setup Option A: Mount Google Drive\n",
                        "# from google.colab import drive\n",
                        "# drive.mount('/content/drive')\n",
                        "\n",
                        "# Manual Setup Option B: Clone from GitHub\n",
                        "# !git clone https://github.com/Tesfay-Hagos/ML-CICIDS-project.git /content/ML-CICIDS-project\n",
                        "\n",
                        "# Manual Setup Option C: Download data_config.py\n",
                        "# !wget -O data_config.py https://raw.githubusercontent.com/Tesfay-Hagos/ML-CICIDS-project/main/data_config.py\n",
                        "\n",
                        "================================================================================\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "# Setup instructions for Google Colab\n",
                "import sys\n",
                "IS_COLAB = 'google.colab' in sys.modules\n",
                "\n",
                "if IS_COLAB:\n",
                "    print(\"=\" * 80)\n",
                "    print(\"GOOGLE COLAB SETUP INSTRUCTIONS\")\n",
                "    print(\"=\" * 80)\n",
                "    print(\"\"\"\n",
                "This notebook is configured to work on Google Colab in two ways:\n",
                "\n",
                "OPTION 1: Using Google Drive (Recommended)\n",
                "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
                "1. Upload the ML-CICIDS-project folder to your Google Drive root\n",
                "2. When prompted in the next cell, authorize Drive access\n",
                "3. The notebook will automatically find the project\n",
                "\n",
                "OPTION 2: Clone from GitHub\n",
                "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
                "1. The notebook will attempt to clone the repo automatically\n",
                "2. This requires the GitHub repo to be public and accessible\n",
                "3. Dataset must be uploaded separately to /content/ML-CICIDS-project/\n",
                "\n",
                "Note: The next cell will handle the setup automatically.\n",
                "If you encounter issues, uncomment and run the manual setup below:\n",
                "\"\"\")\n",
                "    \n",
                "    # Commented manual setup options\n",
                "    print(\"\"\"\n",
                "# Manual Setup Option A: Mount Google Drive\n",
                "# from google.colab import drive\n",
                "# drive.mount('/content/drive')\n",
                "\n",
                "# Manual Setup Option B: Clone from GitHub\n",
                "# !git clone https://github.com/Tesfay-Hagos/ML-CICIDS-project.git /content/ML-CICIDS-project\n",
                "\n",
                "# Manual Setup Option C: Download data_config.py\n",
                "# !wget -O data_config.py https://raw.githubusercontent.com/Tesfay-Hagos/ML-CICIDS-project/main/data_config.py\n",
                "\"\"\")\n",
                "    print(\"=\" * 80 + \"\\n\")\n",
                "else:\n",
                "    print(\"‚úì Running on local machine (not Colab)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "setup_paths",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Running on Colab: True\n",
                        "\n",
                        "üìÇ Setting up Colab environment...\n",
                        "\n",
                        "‚ö†Ô∏è  Could not mount Google Drive: mount failed\n",
                        "\n",
                        "üì• Attempting to clone from GitHub...\n",
                        "\n",
                        "‚ö†Ô∏è  Could not mount Google Drive: mount failed\n",
                        "\n",
                        "üì• Attempting to clone from GitHub...\n",
                        "\n",
                        "‚ö†Ô∏è  Clone succeeded but data_config.py not found\n",
                        "‚ùå ERROR: Could not find project root or data_config.py\n",
                        "\n",
                        "For Colab, you have two options:\n",
                        "1. Mount Google Drive with the project folder\n",
                        "2. Manually upload data_config.py to Colab or update the Google Drive path\n",
                        "\n",
                        "Alternatively, create data_config.py in this Colab cell:\n",
                        "   !wget https://raw.githubusercontent.com/Tesfay-Hagos/ML-CICIDS-project/main/data_config.py\n",
                        "‚ö†Ô∏è  Clone succeeded but data_config.py not found\n",
                        "‚ùå ERROR: Could not find project root or data_config.py\n",
                        "\n",
                        "For Colab, you have two options:\n",
                        "1. Mount Google Drive with the project folder\n",
                        "2. Manually upload data_config.py to Colab or update the Google Drive path\n",
                        "\n",
                        "Alternatively, create data_config.py in this Colab cell:\n",
                        "   !wget https://raw.githubusercontent.com/Tesfay-Hagos/ML-CICIDS-project/main/data_config.py\n"
                    ]
                }
            ],
            "source": [
                "# Base path - adjust if needed\n",
                "import sys\n",
                "import os\n",
                "from pathlib import Path\n",
                "\n",
                "# Check if running on Google Colab\n",
                "IS_COLAB = 'google.colab' in sys.modules\n",
                "\n",
                "print(f\"Running on Colab: {IS_COLAB}\")\n",
                "\n",
                "if IS_COLAB:\n",
                "    # For Colab: Mount Google Drive or clone from GitHub\n",
                "    print(\"\\nüìÇ Setting up Colab environment...\\n\")\n",
                "    \n",
                "    # Try to mount Google Drive first\n",
                "    try:\n",
                "        from google.colab import drive\n",
                "        drive.mount('/content/drive')\n",
                "        # Assume the project is in Google Drive\n",
                "        colab_project_path = Path(\"/content/drive/MyDrive/ML-CICIDS-project\")\n",
                "        if colab_project_path.exists() and (colab_project_path / \"data_config.py\").exists():\n",
                "            project_root = colab_project_path\n",
                "            print(f\"‚úì Found project in Google Drive: {project_root}\")\n",
                "        else:\n",
                "            print(\"‚ö†Ô∏è  Project not found in Google Drive at /content/drive/MyDrive/ML-CICIDS-project\")\n",
                "            project_root = None\n",
                "    except Exception as e:\n",
                "        print(f\"‚ö†Ô∏è  Could not mount Google Drive: {e}\")\n",
                "        project_root = None\n",
                "    \n",
                "    # If not found in Drive, try cloning from GitHub (alternative)\n",
                "    if project_root is None:\n",
                "        print(\"\\nüì• Attempting to clone from GitHub...\\n\")\n",
                "        try:\n",
                "            os.chdir('/content')\n",
                "            os.system('git clone https://github.com/Tesfay-Hagos/ML-CICIDS-project.git')\n",
                "            project_root = Path(\"/content/ML-CICIDS-project\")\n",
                "            if (project_root / \"data_config.py\").exists():\n",
                "                print(f\"‚úì Successfully cloned project: {project_root}\")\n",
                "            else:\n",
                "                print(\"‚ö†Ô∏è  Clone succeeded but data_config.py not found\")\n",
                "                project_root = None\n",
                "        except Exception as e:\n",
                "            print(f\"‚ö†Ô∏è  Clone failed: {e}\")\n",
                "            project_root = None\n",
                "else:\n",
                "    # Local environment (not Colab)\n",
                "    current_dir = Path(os.getcwd())\n",
                "    project_root = None\n",
                "    \n",
                "    # Search current and parent directories for data_config.py\n",
                "    for parent in [current_dir] + list(current_dir.parents):\n",
                "        if (parent / \"data_config.py\").exists():\n",
                "            project_root = parent\n",
                "            break\n",
                "    \n",
                "    # Fallback to known path if automatic discovery fails\n",
                "    if project_root is None:\n",
                "        known_path = Path(\"/home/tesfayh/Artificial_inteligence/ML/CICDS/ML-CICIDS-project/\")\n",
                "        if known_path.exists() and (known_path / \"data_config.py\").exists():\n",
                "            project_root = known_path\n",
                "\n",
                "# Add to system path and import\n",
                "if project_root:\n",
                "    if str(project_root) not in sys.path:\n",
                "        sys.path.insert(0, str(project_root))\n",
                "    print(f\"‚úì Added to path: {project_root}\")\n",
                "    \n",
                "    from data_config import DataConfig\n",
                "    \n",
                "    # Initialize configuration\n",
                "    config = DataConfig(base_path=str(project_root))\n",
                "    config.print_summary()\n",
                "    \n",
                "    # Get file lists from config\n",
                "    flow_files = config.flow_files\n",
                "    ml_files = config.ml_files\n",
                "    \n",
                "    # Display file names\n",
                "    print(\"\\nüìÑ Available files (GeneratedLabelledFlows):\")\n",
                "    for i, f in enumerate(flow_files, 1):\n",
                "        print(f\"   {i}. {f.name}\")\n",
                "    \n",
                "    print(\"\\nüìÑ Available files (MachineLearningCSV):\")\n",
                "    for i, f in enumerate(ml_files, 1):\n",
                "        print(f\"   {i}. {f.name}\")\n",
                "else:\n",
                "    print(\"‚ùå ERROR: Could not find project root or data_config.py\")\n",
                "    print(\"\\nFor Colab, you have two options:\")\n",
                "    print(\"1. Mount Google Drive with the project folder\")\n",
                "    print(\"2. Manually upload data_config.py to Colab or update the Google Drive path\")\n",
                "    print(\"\\nAlternatively, create data_config.py in this Colab cell:\")\n",
                "    print(\"   !wget https://raw.githubusercontent.com/Tesfay-Hagos/ML-CICIDS-project/main/data_config.py\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "dataset_comparison",
            "metadata": {},
            "source": [
                "## 3. Dataset Comparison: Structure and Columns"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "compare_structure",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading sample files for comparison...\n",
                        "\n"
                    ]
                },
                {
                    "ename": "NameError",
                    "evalue": "name 'config' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
                        "\u001b[0;32m/tmp/ipython-input-3833254190.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading sample files for comparison...\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Use config to load files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mflow_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'flow'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mml_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ml'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
                    ]
                }
            ],
            "source": [
                "# Load first file from each dataset to compare structure\n",
                "sample_file = \"Monday-WorkingHours.pcap_ISCX.csv\"\n",
                "\n",
                "print(\"Loading sample files for comparison...\\n\")\n",
                "# Use config to load files\n",
                "flow_sample = config.load_file(sample_file, dataset='flow', nrows=1000)\n",
                "ml_sample = config.load_file(sample_file, dataset='ml', nrows=1000)\n",
                "\n",
                "print(\"=\" * 80)\n",
                "print(\"DATASET STRUCTURE COMPARISON\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "print(f\"\\nüìä GeneratedLabelledFlows:\")\n",
                "print(f\"   - Shape: {flow_sample.shape}\")\n",
                "print(f\"   - Columns: {len(flow_sample.columns)}\")\n",
                "print(f\"   - Memory: {flow_sample.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
                "\n",
                "print(f\"\\nüìä MachineLearningCSV:\")\n",
                "print(f\"   - Shape: {ml_sample.shape}\")\n",
                "print(f\"   - Columns: {len(ml_sample.columns)}\")\n",
                "print(f\"   - Memory: {ml_sample.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
                "\n",
                "# Column differences\n",
                "flow_cols = set(flow_sample.columns)\n",
                "ml_cols = set(ml_sample.columns)\n",
                "\n",
                "common_cols = flow_cols & ml_cols\n",
                "flow_only = flow_cols - ml_cols\n",
                "ml_only = ml_cols - flow_cols\n",
                "\n",
                "print(f\"\\nüîç Column Analysis:\")\n",
                "print(f\"   - Common columns: {len(common_cols)}\")\n",
                "print(f\"   - Only in GeneratedLabelledFlows: {len(flow_only)}\")\n",
                "print(f\"   - Only in MachineLearningCSV: {len(ml_only)}\")\n",
                "\n",
                "if flow_only:\n",
                "    print(f\"\\n   Columns ONLY in GeneratedLabelledFlows:\")\n",
                "    for col in sorted(flow_only):\n",
                "        print(f\"      ‚Ä¢ {col}\")\n",
                "\n",
                "if ml_only:\n",
                "    print(f\"\\n   Columns ONLY in MachineLearningCSV:\")\n",
                "    for col in sorted(ml_only):\n",
                "        print(f\"      ‚Ä¢ {col}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "visualize_columns",
            "metadata": {},
            "source": [
                "### Visualize Column Differences"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "plot_column_comparison",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create visualization of column distribution\n",
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
                "\n",
                "# Pie chart showing column composition\n",
                "column_data = [len(common_cols), len(flow_only), len(ml_only)]\n",
                "labels = ['Common Columns', 'Flow Only', 'ML Only']\n",
                "colors = ['#2ecc71', '#3498db', '#e74c3c']\n",
                "explode = (0.05, 0.05, 0.05)\n",
                "\n",
                "ax1.pie(column_data, labels=labels, colors=colors, autopct='%1.1f%%', \n",
                "        startangle=90, explode=explode, shadow=True)\n",
                "ax1.set_title('Column Distribution Across Datasets', fontsize=14, fontweight='bold')\n",
                "\n",
                "# Bar chart comparing total columns\n",
                "datasets = ['GeneratedLabelledFlows', 'MachineLearningCSV']\n",
                "column_counts = [len(flow_sample.columns), len(ml_sample.columns)]\n",
                "bars = ax2.bar(datasets, column_counts, color=['#3498db', '#e74c3c'], alpha=0.7, edgecolor='black')\n",
                "\n",
                "# Add value labels on bars\n",
                "for bar in bars:\n",
                "    height = bar.get_height()\n",
                "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
                "            f'{int(height)}',\n",
                "            ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
                "\n",
                "ax2.set_ylabel('Number of Columns', fontsize=12)\n",
                "ax2.set_title('Total Columns Per Dataset', fontsize=14, fontweight='bold')\n",
                "ax2.grid(axis='y', alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "data_overview",
            "metadata": {},
            "source": [
                "## 4. Data Overview and Statistics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "load_full_monday",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load full Monday dataset for detailed analysis\n",
                "print(\"Loading Monday dataset (full) for detailed analysis...\\n\")\n",
                "\n",
                "# Using MachineLearningCSV as it's preprocessed for ML\n",
                "monday_data = config.load_file(sample_file, dataset='ml')\n",
                "\n",
                "print(\"‚úì Data loaded successfully!\\n\")\n",
                "print(\"=\" * 80)\n",
                "print(\"DATASET OVERVIEW\")\n",
                "print(\"=\" * 80)\n",
                "print(f\"Shape: {monday_data.shape[0]:,} rows √ó {monday_data.shape[1]} columns\")\n",
                "print(f\"Memory usage: {monday_data.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
                "print(f\"\\nFirst few rows:\")\n",
                "display(monday_data.head())\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"DATA TYPES\")\n",
                "print(\"=\"*80)\n",
                "print(monday_data.dtypes.value_counts())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "basic_stats",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Statistical summary\n",
                "print(\"üìä Statistical Summary of Numerical Features:\\n\")\n",
                "display(monday_data.describe())"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "missing_data",
            "metadata": {},
            "source": [
                "## 5. Data Quality Assessment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "check_missing",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check for missing values\n",
                "missing_data = monday_data.isnull().sum()\n",
                "missing_percent = (missing_data / len(monday_data)) * 100\n",
                "missing_df = pd.DataFrame({\n",
                "    'Missing Count': missing_data,\n",
                "    'Percentage': missing_percent\n",
                "})\n",
                "missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
                "\n",
                "print(\"=\" * 80)\n",
                "print(\"MISSING DATA ANALYSIS\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "if len(missing_df) > 0:\n",
                "    print(f\"\\n‚ö†Ô∏è  Found {len(missing_df)} columns with missing values:\\n\")\n",
                "    display(missing_df.head(20))\n",
                "else:\n",
                "    print(\"\\n‚úì No missing values found!\")\n",
                "\n",
                "# Check for infinite values in numerical columns\n",
                "numerical_cols = monday_data.select_dtypes(include=[np.number]).columns\n",
                "inf_counts = {}\n",
                "\n",
                "for col in numerical_cols:\n",
                "    inf_count = np.isinf(monday_data[col]).sum()\n",
                "    if inf_count > 0:\n",
                "        inf_counts[col] = inf_count\n",
                "\n",
                "if inf_counts:\n",
                "    print(f\"\\n‚ö†Ô∏è  Found infinite values in {len(inf_counts)} columns:\")\n",
                "    for col, count in sorted(inf_counts.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
                "        print(f\"   ‚Ä¢ {col}: {count:,} infinite values\")\n",
                "else:\n",
                "    print(\"\\n‚úì No infinite values found!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "visualize_missing",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize missing data\n",
                "if len(missing_df) > 0:\n",
                "    fig, ax = plt.subplots(figsize=(12, 6))\n",
                "    \n",
                "    top_missing = missing_df.head(15)\n",
                "    bars = ax.barh(range(len(top_missing)), top_missing['Percentage'], color='coral', edgecolor='black')\n",
                "    ax.set_yticks(range(len(top_missing)))\n",
                "    ax.set_yticklabels(top_missing.index)\n",
                "    ax.set_xlabel('Missing Percentage (%)', fontsize=12)\n",
                "    ax.set_title('Top 15 Columns with Missing Data', fontsize=14, fontweight='bold')\n",
                "    ax.grid(axis='x', alpha=0.3)\n",
                "    \n",
                "    # Add percentage labels\n",
                "    for i, bar in enumerate(bars):\n",
                "        width = bar.get_width()\n",
                "        ax.text(width, bar.get_y() + bar.get_height()/2.,\n",
                "               f'{width:.2f}%',\n",
                "               ha='left', va='center', fontsize=9)\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "else:\n",
                "    print(\"‚úì No missing data to visualize!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "label_analysis",
            "metadata": {},
            "source": [
                "## 6. Label Distribution Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "analyze_labels",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze label distribution\n",
                "label_col = ' Label' if ' Label' in monday_data.columns else 'Label'\n",
                "\n",
                "label_counts = monday_data[label_col].value_counts()\n",
                "label_percentages = monday_data[label_col].value_counts(normalize=True) * 100\n",
                "\n",
                "label_summary = pd.DataFrame({\n",
                "    'Count': label_counts,\n",
                "    'Percentage': label_percentages\n",
                "})\n",
                "\n",
                "print(\"=\" * 80)\n",
                "print(\"LABEL DISTRIBUTION (Monday Dataset)\")\n",
                "print(\"=\" * 80)\n",
                "print(f\"\\nTotal unique labels: {len(label_counts)}\\n\")\n",
                "display(label_summary)\n",
                "\n",
                "# Check for class imbalance\n",
                "if len(label_counts) > 1:\n",
                "    imbalance_ratio = label_counts.max() / label_counts.min()\n",
                "    print(f\"\\n‚öñÔ∏è  Class Imbalance Ratio: {imbalance_ratio:.2f}:1\")\n",
                "    if imbalance_ratio > 10:\n",
                "        print(\"   ‚ö†Ô∏è  Significant class imbalance detected! Consider using:\")\n",
                "        print(\"      ‚Ä¢ SMOTE (Synthetic Minority Over-sampling)\")\n",
                "        print(\"      ‚Ä¢ Class weights in model training\")\n",
                "        print(\"      ‚Ä¢ Stratified sampling\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "plot_labels",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize label distribution\n",
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
                "\n",
                "# Bar plot\n",
                "colors_list = plt.cm.Set3(range(len(label_counts)))\n",
                "bars = ax1.bar(range(len(label_counts)), label_counts.values, color=colors_list, edgecolor='black', alpha=0.8)\n",
                "ax1.set_xticks(range(len(label_counts)))\n",
                "ax1.set_xticklabels(label_counts.index, rotation=45, ha='right')\n",
                "ax1.set_ylabel('Count', fontsize=12)\n",
                "ax1.set_title('Label Distribution (Count)', fontsize=14, fontweight='bold')\n",
                "ax1.grid(axis='y', alpha=0.3)\n",
                "\n",
                "# Add count labels on bars\n",
                "for bar in bars:\n",
                "    height = bar.get_height()\n",
                "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
                "            f'{int(height):,}',\n",
                "            ha='center', va='bottom', fontsize=9, rotation=0)\n",
                "\n",
                "# Pie chart\n",
                "if len(label_counts) <= 10:  # Only show pie chart if not too many labels\n",
                "    wedges, texts, autotexts = ax2.pie(label_counts.values, labels=label_counts.index, \n",
                "                                         autopct='%1.1f%%', startangle=90, \n",
                "                                         colors=colors_list, \n",
                "                                         explode=[0.05] * len(label_counts))\n",
                "    ax2.set_title('Label Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
                "    \n",
                "    # Make percentage text more readable\n",
                "    for autotext in autotexts:\n",
                "        autotext.set_color('white')\n",
                "        autotext.set_fontweight('bold')\n",
                "else:\n",
                "    # If too many labels, show log scale bar plot\n",
                "    label_counts.plot(kind='bar', ax=ax2, color=colors_list, edgecolor='black', alpha=0.8, logy=True)\n",
                "    ax2.set_title('Label Distribution (Log Scale)', fontsize=14, fontweight='bold')\n",
                "    ax2.set_ylabel('Count (log scale)', fontsize=12)\n",
                "    ax2.set_xlabel('Label', fontsize=12)\n",
                "    ax2.tick_params(axis='x', rotation=45)\n",
                "    ax2.grid(axis='y', alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "all_files_labels",
            "metadata": {},
            "source": [
                "## 7. Complete Dataset Label Distribution"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "analyze_all_files",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze labels across all files\n",
                "print(\"Loading all files to analyze complete label distribution...\\n\")\n",
                "print(\"This may take a moment...\\n\")\n",
                "\n",
                "all_labels = []\n",
                "file_info = []\n",
                "\n",
                "for csv_file in ml_files:\n",
                "    print(f\"Processing: {csv_file.name}\")\n",
                "    try:\n",
                "        df = pd.read_csv(csv_file)\n",
                "        labels = df[label_col].value_counts()\n",
                "        \n",
                "        file_info.append({\n",
                "            'File': csv_file.name,\n",
                "            'Total Rows': len(df),\n",
                "            'Unique Labels': len(labels),\n",
                "            'Labels': ', '.join(labels.index.tolist())\n",
                "        })\n",
                "        \n",
                "        all_labels.extend(df[label_col].tolist())\n",
                "    except Exception as e:\n",
                "        print(f\"   ‚ö†Ô∏è  Error: {e}\")\n",
                "\n",
                "# Create summary DataFrame\n",
                "file_summary = pd.DataFrame(file_info)\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"FILE-LEVEL SUMMARY\")\n",
                "print(\"=\"*80)\n",
                "display(file_summary)\n",
                "\n",
                "# Overall label distribution\n",
                "overall_labels = pd.Series(all_labels).value_counts()\n",
                "overall_percentages = pd.Series(all_labels).value_counts(normalize=True) * 100\n",
                "\n",
                "overall_summary = pd.DataFrame({\n",
                "    'Count': overall_labels,\n",
                "    'Percentage': overall_percentages\n",
                "})\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"OVERALL LABEL DISTRIBUTION (All Files Combined)\")\n",
                "print(\"=\"*80)\n",
                "print(f\"Total samples: {len(all_labels):,}\")\n",
                "print(f\"Unique labels: {len(overall_labels)}\\n\")\n",
                "display(overall_summary)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "plot_overall_labels",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize overall label distribution\n",
                "fig, ax = plt.subplots(figsize=(14, 7))\n",
                "\n",
                "colors_overall = plt.cm.tab20(range(len(overall_labels)))\n",
                "bars = ax.bar(range(len(overall_labels)), overall_labels.values, \n",
                "              color=colors_overall, edgecolor='black', alpha=0.8)\n",
                "\n",
                "ax.set_xticks(range(len(overall_labels)))\n",
                "ax.set_xticklabels(overall_labels.index, rotation=45, ha='right', fontsize=10)\n",
                "ax.set_ylabel('Count', fontsize=12, fontweight='bold')\n",
                "ax.set_xlabel('Attack Type', fontsize=12, fontweight='bold')\n",
                "ax.set_title('Complete CICIDS2017 Dataset - Label Distribution (All Files)', \n",
                "             fontsize=16, fontweight='bold', pad=20)\n",
                "ax.grid(axis='y', alpha=0.3)\n",
                "ax.set_yscale('log')  # Log scale for better visibility\n",
                "\n",
                "# Add count labels\n",
                "for i, bar in enumerate(bars):\n",
                "    height = bar.get_height()\n",
                "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
                "           f'{int(overall_labels.values[i]):,}',\n",
                "           ha='center', va='bottom', fontsize=8, rotation=45)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "feature_analysis",
            "metadata": {},
            "source": [
                "## 8. Feature Distribution Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "analyze_features",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Select numerical features for analysis (excluding label)\n",
                "feature_cols = [col for col in monday_data.select_dtypes(include=[np.number]).columns \n",
                "                if col != label_col]\n",
                "\n",
                "print(f\"Analyzing {len(feature_cols)} numerical features...\\n\")\n",
                "\n",
                "# Sample of key features to visualize\n",
                "key_features = [\n",
                "    'Flow Duration',\n",
                "    ' Total Fwd Packets',\n",
                "    ' Total Backward Packets',\n",
                "    'Total Length of Fwd Packets',\n",
                "    ' Total Length of Bwd Packets',\n",
                "    ' Flow Bytes/s',\n",
                "    ' Flow Packets/s'\n",
                "]\n",
                "\n",
                "# Filter to features that exist\n",
                "available_features = [f for f in key_features if f in monday_data.columns]\n",
                "\n",
                "if len(available_features) >= 4:\n",
                "    # Plot distributions\n",
                "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
                "    axes = axes.ravel()\n",
                "    \n",
                "    for i, feature in enumerate(available_features[:4]):\n",
                "        # Remove infinite values for visualization\n",
                "        data = monday_data[feature].replace([np.inf, -np.inf], np.nan).dropna()\n",
                "        \n",
                "        axes[i].hist(data, bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
                "        axes[i].set_xlabel(feature, fontsize=11)\n",
                "        axes[i].set_ylabel('Frequency', fontsize=11)\n",
                "        axes[i].set_title(f'Distribution: {feature}', fontsize=12, fontweight='bold')\n",
                "        axes[i].grid(alpha=0.3)\n",
                "        \n",
                "        # Add statistics\n",
                "        mean_val = data.mean()\n",
                "        median_val = data.median()\n",
                "        axes[i].axvline(mean_val, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_val:.2e}')\n",
                "        axes[i].axvline(median_val, color='green', linestyle='--', linewidth=2, label=f'Median: {median_val:.2e}')\n",
                "        axes[i].legend()\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "else:\n",
                "    print(\"Not enough features available for visualization.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "correlation",
            "metadata": {},
            "source": [
                "## 9. Feature Correlation Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "correlation_matrix",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate correlation matrix for a subset of features\n",
                "print(\"Computing correlation matrix...\\n\")\n",
                "\n",
                "# Select subset of features (to avoid overwhelming visualization)\n",
                "sample_features = monday_data[available_features[:10]].replace([np.inf, -np.inf], np.nan)\n",
                "correlation_matrix = sample_features.corr()\n",
                "\n",
                "# Plot heatmap\n",
                "fig, ax = plt.subplots(figsize=(12, 10))\n",
                "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
                "            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8},\n",
                "            ax=ax)\n",
                "ax.set_title('Feature Correlation Matrix (Sample Features)', fontsize=14, fontweight='bold', pad=20)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Find highly correlated features\n",
                "high_corr = []\n",
                "for i in range(len(correlation_matrix.columns)):\n",
                "    for j in range(i+1, len(correlation_matrix.columns)):\n",
                "        if abs(correlation_matrix.iloc[i, j]) > 0.8:\n",
                "            high_corr.append({\n",
                "                'Feature 1': correlation_matrix.columns[i],\n",
                "                'Feature 2': correlation_matrix.columns[j],\n",
                "                'Correlation': correlation_matrix.iloc[i, j]\n",
                "            })\n",
                "\n",
                "if high_corr:\n",
                "    print(\"\\n‚ö†Ô∏è  Highly correlated features (|r| > 0.8):\")\n",
                "    high_corr_df = pd.DataFrame(high_corr).sort_values('Correlation', key=abs, ascending=False)\n",
                "    display(high_corr_df)\n",
                "    print(\"\\nüí° Consider removing one feature from each pair to reduce multicollinearity.\")\n",
                "else:\n",
                "    print(\"\\n‚úì No highly correlated features found (|r| > 0.8)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "recommendations",
            "metadata": {},
            "source": [
                "## 10. Dataset Selection Recommendation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "final_recommendation",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 80)\n",
                "print(\"DATASET SELECTION RECOMMENDATION\")\n",
                "print(\"=\" * 80)\n",
                "print(\"\"\"\n",
                "Based on the analysis, here are the recommendations:\n",
                "\n",
                "üìå FOR MACHINE LEARNING MODEL TRAINING:\n",
                "   ‚úì USE: MachineLearningCSV/MachineLearningCVE\n",
                "   \n",
                "   Reasons:\n",
                "   ‚Ä¢ Preprocessed and optimized for ML algorithms\n",
                "   ‚Ä¢ Removes identifying information (IPs, ports, timestamps)\n",
                "   ‚Ä¢ Focuses on statistical flow features\n",
                "   ‚Ä¢ Privacy-preserving (no personal/network identifiers)\n",
                "   ‚Ä¢ Smaller memory footprint\n",
                "   ‚Ä¢ Industry standard for intrusion detection research\n",
                "\n",
                "üìå FOR NETWORK FORENSICS & DETAILED ANALYSIS:\n",
                "   ‚úì USE: GeneratedLabelledFlows\n",
                "   \n",
                "   Reasons:\n",
                "   ‚Ä¢ Contains complete flow information\n",
                "   ‚Ä¢ Includes Flow ID, Source/Dest IPs, Ports, Timestamps\n",
                "   ‚Ä¢ Useful for tracking specific flows\n",
                "   ‚Ä¢ Better for investigating attack patterns\n",
                "   ‚Ä¢ Correlate with original PCAP files\n",
                "\n",
                "üìä DATASET STATISTICS:\n",
                "   ‚Ä¢ Total Samples: {:,}\n",
                "   ‚Ä¢ Unique Attack Types: {}\n",
                "   ‚Ä¢ Features (ML version): {}\n",
                "   ‚Ä¢ Class Imbalance: Present (consider SMOTE or class weights)\n",
                "\n",
                "‚ö†Ô∏è  KEY CONSIDERATIONS:\n",
                "   ‚Ä¢ Significant class imbalance exists - use appropriate techniques\n",
                "   ‚Ä¢ Some features contain infinite values - handle during preprocessing\n",
                "   ‚Ä¢ High correlation between some features - consider dimensionality reduction\n",
                "   ‚Ä¢ Stratified sampling recommended for train/test split\n",
                "\n",
                "üéØ NEXT STEPS:\n",
                "   1. Data Preprocessing (handle infinities, normalize features)\n",
                "   2. Feature Selection/Engineering\n",
                "   3. Handle Class Imbalance (SMOTE, class weights)\n",
                "   4. Train/Test Split (stratified)\n",
                "   5. Model Selection and Training\n",
                "   6. Evaluation with appropriate metrics (F1, Precision, Recall)\n",
                "\"\"\".format(\n",
                "    len(all_labels),\n",
                "    len(overall_labels),\n",
                "    len(ml_sample.columns)\n",
                "))\n",
                "\n",
                "print(\"=\" * 80)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "export_summary",
            "metadata": {},
            "source": [
                "## 11. Export Analysis Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "export_results",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create summary report\n",
                "summary_report = {\n",
                "    'analysis_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
                "    'total_samples': len(all_labels),\n",
                "    'unique_labels': len(overall_labels),\n",
                "    'label_distribution': overall_summary.to_dict(),\n",
                "    'files_analyzed': len(ml_files),\n",
                "    'file_summary': file_summary.to_dict(),\n",
                "    'recommended_dataset': 'MachineLearningCSV/MachineLearningCVE',\n",
                "    'features_count': len(ml_sample.columns)\n",
                "}\n",
                "\n",
                "# Save to JSON\n",
                "import json\n",
                "output_path = config.base_path / 'data_analysis_summary.json'\n",
                "\n",
                "with open(output_path, 'w') as f:\n",
                "    json.dump(summary_report, f, indent=2, default=str)\n",
                "\n",
                "print(f\"‚úì Analysis summary saved to: {output_path}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
